{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2882fd-3025-4236-a302-2d42bd130888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse,urljoin\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d2909d-6396-41d9-b712-ac0a432110e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://inecelectionresults.ng/elections/types'\n",
    "STATES = [\n",
    "    'ABIA','ADAMAWA','AKWA IBOM','ANAMBRA',\n",
    "    'BAUCHI','BAYELSA','BENUE','BORNO',\n",
    "     'CROSS RIVER','DELTA','EBONYI','EDO',\n",
    "     'EKITI','ENUGU','FCT','GOMBE','IMO',\n",
    "    'JIGAWA','KADUNA','KANO','KATSINA',\n",
    "     'KEBBI','KOGI','KWARA','LAGOS',\n",
    "    'NASARAWA','NIGER','OGUN','ONDO',\n",
    "     'OSUN', 'OYO','PLATEAU','RIVERS',\n",
    "     'SOKOTO','TARABA','YOBE','ZAMFARA' \n",
    "]\n",
    "# driver = webdriver.Firefox()\n",
    "# driver.get(url)\n",
    "# time.sleep(1)\n",
    "# htmlSource = driver.page_source\n",
    "# soup = BeautifulSoup(htmlSource, 'html.parser')\n",
    "\n",
    "# c = urlparse(url)\n",
    "# c.hostname\n",
    "# c.scheme\n",
    "# urljoin(c.scheme,c.hostname)\n",
    "# f'{c.scheme}://{c.hostname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c062665-f6fb-4b89-ba96-eaaba13d581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# imgURL = \"https://inec-cvr-cache.s3.eu-west-1.amazonaws.com/cached/results/652109/result_23607_1678026019_thumb.jpg\"\n",
    "# img_url = urlparse(imgURL)\n",
    "# # urllib.request.urlretrieve(imgURL,'result_1678026019_thumb.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b29bd8-f46c-4bae-8897-3bb8c805e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9f8848-f989-43a2-ae9b-bb5098b9b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    driver.refresh()\n",
    "    time.sleep(3)\n",
    "    driver.refresh()\n",
    "    time.sleep(5)\n",
    "    htmlSource = driver.page_source\n",
    "    page = re.sub('<!--*>','',htmlSource)\n",
    "    soup = BeautifulSoup(page, 'html.parser') \n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec6a385-ff97-4c3a-903c-4c993fbdee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = get_page(\"https://inecelectionresults.ng/elections/63f8f25b594e164f8146a213/context/pus/lga/5f0f397f4d89fc3a883de102/ward/5f0f3aa18f77bb3acad0905c\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63df19f3-9ec9-4627-bb8d-785fbdbd9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page2 = get_page(\"https://inecelectionresults.ng/elections/63f8f25b594e164f8146a213/context/pus/lga/5f0f397c4d89fc3a883de0d2/ward/5f0f3a568f77bb3acad08e62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75555a04-a1a6-4a6d-8eac-cfb4eebf620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# # lists = page2.findAll('a', {'href': re.compile(r\"(https.*inec.*)\")} )\n",
    "# # print(len(lists))\n",
    "# h2 = page2.findAll('h2')[-1]\n",
    "# all_divs = h2.next_sibling.next_sibling.contents\n",
    "# # print(all_divs[-1] == ' ')\n",
    "\n",
    "# not_available = [div for div in all_divs if not div.a]\n",
    "# details = [[div.get_text() for div in divs.findAll('div')[-2:] ]for divs in not_available]\n",
    "# for divds in not_available:\n",
    "#     print('hello')\n",
    "    # print([div.get_text() for div in divs])\n",
    "    # print([div.get_text() for div in divds])\n",
    "# not_available[0].findAll('div')[-2:]\n",
    "# for link in lists:\n",
    "#     img_link = link['href']\n",
    "#     print(img_link)\n",
    "#     print()\n",
    "    # divs = link.parent.previous_sibling.findAll('div')\n",
    "    # details = [div.get_text() for div in divs]\n",
    "    # print(img_link)\n",
    "    # print(details)\n",
    "    \n",
    "    # break\n",
    "# page2.find('div','text'==details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5551c8e9-e441-424e-aed8-4f1f846746be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url , type = \"presidential\",home=True,date=None,querry=None):\n",
    "    \n",
    "    links = []\n",
    "    absolute_url = urlparse(url)\n",
    "    base_url = f'{absolute_url.scheme}://{absolute_url.hostname}'\n",
    "\n",
    "    if querry:\n",
    "        soup = get_page(url)\n",
    "        for link in soup.findAll('a'):\n",
    "            if not link.get('href'):\n",
    "                continue\n",
    "            \n",
    "            if querry in link['href']:\n",
    "                tuple_data = (link.get_text(),f\"{base_url}{link['href']}\")\n",
    "                links.append(tuple_data) \n",
    "        return links\n",
    "    \n",
    "    if home:\n",
    "        querry = f'{type} election'.lower()\n",
    "    else:\n",
    "        querry = f'{type} election - {date} - {type}'.lower()\n",
    "\n",
    "    soup = get_page(url)\n",
    "    for link in soup.findAll('a'):\n",
    "        if link.get_text().lower() == querry:\n",
    "            tuple_data = (link.get_text(),f\"{base_url}{link['href']}\")\n",
    "            links.append(tuple_data) \n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cda69fb-562a-4f73-a89b-a672fdcc48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SATE_links = []\n",
    "# print(len(SATE_links) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16dd105f-d395-4302-8e0e-df5cb3489d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sttt = \"<!----> absndddjjdj\"\n",
    "# re.sub('<!--*>','',sttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6beb6b7c-ee13-4280-8b02-3253a7833c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(name,key=None):\n",
    "    try:\n",
    "        with open(name,'r') as openfile:\n",
    "            data = json.load(openfile)\n",
    "            if key:\n",
    "                return data[key]\n",
    "            return data\n",
    "    except (FileNotFoundError, KeyError)as e:\n",
    "        if key:\n",
    "            return []\n",
    "        return {}\n",
    "\n",
    "def write_json(data,name,key=None):\n",
    "    if key:\n",
    "        initial_data = read_json(name)\n",
    "        initial_data[key] = data\n",
    "        data = initial_data\n",
    "    data = json.dumps(data)\n",
    "    with open(name,'w') as outfile:\n",
    "        outfile.write(data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab0f1a1b-f216-419b-8e9b-6484ea2c1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=read_json('presidential_state.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b456b394-db4e-4432-babc-57cbbb8f85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993fee2a-39da-42ba-b584-8d08d5214c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_levels(links_list,file_base_name,type,querry):\n",
    "#     fulldata = read_json(file_base_name)   \n",
    "#     for level_name,level_link in links_list:\n",
    "#         print(level_name)\n",
    "#         path = urlparse(level_link)\n",
    "#         if not fulldata.get(level_name) or len(fulldata.get(level_name)) < 1 :\n",
    "#             data = get_links(level_link,type,False,querry=path.path)\n",
    "#             print(len(data))\n",
    "#             write_json(data,file_base_name,level_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7945401-fff6-458e-a9a7-1a021944d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type = \"presidential\"\n",
    "# date = '2023-02-25'\n",
    "# file_base_name = f'{type}_{date}.json'\n",
    "\n",
    "# len(read_json(file_base_name,'lgs'))\n",
    "# get_data = read_json(file_base_name)\n",
    "# keys= get_data.keys()\n",
    "# print(len(keys))\n",
    "# keys= set(keys)\n",
    "# print(len(keys))\n",
    "\n",
    "# not_needed = set(STATES + ['states'])\n",
    "# print(len(not_needed))\n",
    "\n",
    "# needed = keys - not_needed\n",
    "# print(len(needed))\n",
    "# print(len(get_data['lgs']))\n",
    "# # print(list(needed))\n",
    "# # write_json(list(needed),file_base_name,'lgs')\n",
    "\n",
    "# needed - set(get_data['lgs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d76b89-72d7-4936-b9a3-d9459c4e3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(url,date,type=\"presidential\",state=None,states=[]):\n",
    "    print('running')\n",
    "    file_base_name = f'{type}_{date}.json'\n",
    "    \n",
    "    states_links = read_json(file_base_name,'states')\n",
    "    if len(states_links) == 0:\n",
    "        [url] = get_links(url,type)\n",
    "        [url] = get_links(url[1],type,False,date)\n",
    "        url = url[1]\n",
    "   \n",
    "    if type == \"presidential\" and len(states_links) == 0 :\n",
    "        absolute_url = urlparse(url)\n",
    "        base_url = f'{absolute_url.scheme}://{absolute_url.hostname}'\n",
    "        id = absolute_url.path.split('/')[-1]\n",
    "\n",
    "        for num in range(1,38):\n",
    "            link = f'{base_url}/elections/{id}?state={num}'\n",
    "            states_links.append((STATES[num-1],link))\n",
    "\n",
    "        write_json(states_links,file_base_name,'states')\n",
    "\n",
    "    # get_levels(state_link,file_base_name,type,querry)\n",
    "    fulldata = read_json(file_base_name)   \n",
    "    for state_name,state_link in states_links:\n",
    "        path = urlparse(state_link)\n",
    "        if not fulldata.get(state_name) or len(fulldata.get(state_name)) < 1 :\n",
    "            lgs = get_links(state_link,type,False,querry=path.path)\n",
    "            write_json(lgs,file_base_name,state_name)\n",
    "\n",
    "    fulldata = read_json(file_base_name)\n",
    "    lgs_list = []\n",
    "    if not fulldata.get('lgs'):\n",
    "        write_json([],file_base_name,'lgs')\n",
    "    else:\n",
    "        lgs_list = fulldata['lgs']\n",
    "    for state in STATES:\n",
    "        for lg_name,lg_link in fulldata[state]:\n",
    "            path = urlparse(lg_link)\n",
    "            querry = '/'.join(path.path.split('/')[0:3])\n",
    "            if not fulldata.get(lg_name) or len(fulldata.get(lg_name)) < 1 :\n",
    "                lgs_list.append(lg_name)\n",
    "                wards = get_links(lg_link,type,False,querry=querry)\n",
    "                write_json(wards,file_base_name,lg_name)\n",
    "                write_json(lgs_list,file_base_name,'lgs')\n",
    "                \n",
    "    print('done')\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6c0a2f-cff3-4208-88e9-d20a535ca286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrap(url,'2023-02-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dedd839-5bee-41b7-9612-6415cb456ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_level(type,date,level='lgs',needed= \"all\",download=True,recheck=False):\n",
    "    file_base_name = f'{type}_{date}.json'\n",
    "    downloaded ={}\n",
    "    # full_data = None\n",
    "    # lg_names = None\n",
    "    if level == \"lgs\":\n",
    "        full_data = read_json(file_base_name)\n",
    "        lg_names = full_data['lgs']\n",
    "    if needed == \"all\":\n",
    "        lg_names = lg_names\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if full_data.get('downloads'):\n",
    "        downloaded = full_data['downloads'].copy()\n",
    "    for lg_name in lg_names:\n",
    "        print(lg_name)\n",
    "        try:\n",
    "            if downloaded.get(lg_name):\n",
    "                if downloaded[lg_name]['status'] in ['completed','ran_through']:\n",
    "                    continue\n",
    "            else:\n",
    "                downloaded[lg_name] = {'status': 'in_progress','missing':False}  \n",
    "        except keyError:\n",
    "            downloaded[lg_name] = {'status': 'in_progress','missing':False}\n",
    "                \n",
    "            \n",
    "        for ward_name,ward_url in full_data[lg_name]:\n",
    "            print(ward_name)\n",
    "            try:\n",
    "                if downloaded[lg_name][ward_name]['recheck'] == False:\n",
    "                    continue\n",
    "                elif downloaded[lg_name][ward_name]['missing'] == False:\n",
    "                    continue\n",
    "            except (AttributeError,KeyError) as e:\n",
    "                pass\n",
    "            print(f'continue with {ward_name}')\n",
    "            page = get_page(ward_url)\n",
    "            h2 = page.findAll('h2')[-1]\n",
    "            all_divs = h2.next_sibling.next_sibling.contents\n",
    "            not_available = [div for div in all_divs if not div.a]\n",
    "            not_available = [[div.get_text() for div in divs.findAll('div')[-2:] ]for divs in not_available]\n",
    "            downloaded[lg_name][ward_name] = {'unavailable':not_available,'missing':False,'recheck':recheck}\n",
    "            if len(not_available) < 0:\n",
    "                downloaded[lg_name]['missing'] = True\n",
    "                downloaded[lg_name][ward_name]['missing'] = True\n",
    "                    \n",
    "            lists = page.findAll('a', {'href': re.compile(r\"(https.*inec.*results.*)\")})\n",
    "\n",
    "            for link in lists:\n",
    "                img_link = link['href']\n",
    "                divs = link.parent.previous_sibling.findAll('div')\n",
    "                details = [div.get_text() for div in divs]\n",
    "                key = f'{details[1]}-->{details[0]}'\n",
    "                # print(img_link)\n",
    "                try:\n",
    "                    downloaded[lg_name][ward_name][key]\n",
    "                    continue\n",
    "                except KeyError:\n",
    "                    downloaded[lg_name][ward_name][key] = img_link\n",
    "                    if download:\n",
    "                        Path(\"./unstorted\").mkdir(parents=True, exist_ok=True)\n",
    "                        img_url = urlparse(img_link)\n",
    "                        path = f\"./unstorted/{img_url.path.split('/')[-1]}\"\n",
    "                        try:\n",
    "                            urllib.request.urlretrieve(img_link,path)\n",
    "                        except urllib.error.HTTPError:\n",
    "                            del downloaded[lg_name][ward_name][key]\n",
    "                            # print(img_link)\n",
    "                        \n",
    "                    write_json(downloaded,file_base_name,'downloads')\n",
    "        if downloaded[lg_name]['missing'] == False:\n",
    "            print('no missing')\n",
    "            downloaded[lg_name]['status'] = \"completed\"\n",
    "        else:\n",
    "            downloaded[lg_name]['status'] = \"ran_through\"\n",
    "        print(lg_name)\n",
    "        print(downloaded[lg_name]['status'])\n",
    "        write_json(downloaded,file_base_name,'downloads')\n",
    "            \n",
    "\n",
    "                                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dbd1236-3478-4783-8701-70b7ddc8791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDOKWA WEST (LGA: 12) - DELTA\n",
      "AFIKPO NORTH (LGA: 02) - EBONYI\n",
      "OGBIA (LGA: 05) - BAYELSA\n",
      "OGBIA (Ward: 01) - BAYELSA\n",
      "OTUOKPOTI (Ward: 02) - BAYELSA\n",
      "OLOGI (Ward: 03) - BAYELSA\n",
      "ANYAMA (Ward: 04) - BAYELSA\n",
      "OKODI (Ward: 05) - BAYELSA\n",
      "OTUASEGA (Ward: 06) - BAYELSA\n",
      "EMEYAL (Ward: 07) - BAYELSA\n",
      "IMIRINGI (Ward: 08) - BAYELSA\n",
      "KOLO (Ward: 09) - BAYELSA\n",
      "OLOIBIRI (Ward: 10) - BAYELSA\n",
      "continue with OLOIBIRI (Ward: 10) - BAYELSA\n",
      "OPUME (Ward: 11) - BAYELSA\n",
      "continue with OPUME (Ward: 11) - BAYELSA\n",
      "OTAKEME (Ward: 12) - BAYELSA\n",
      "continue with OTAKEME (Ward: 12) - BAYELSA\n",
      "OTUABULA (Ward: 13) - BAYELSA\n",
      "continue with OTUABULA (Ward: 13) - BAYELSA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdownload_from_level\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpresidential\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 40\u001b[0m, in \u001b[0;36mdownload_from_level\u001b[1;34m(type, date, level, needed, download, recheck)\u001b[0m\n\u001b[0;32m     38\u001b[0m page \u001b[38;5;241m=\u001b[39m get_page(ward_url)\n\u001b[0;32m     39\u001b[0m h2 \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m all_divs \u001b[38;5;241m=\u001b[39m \u001b[43mh2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_sibling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_sibling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontents\u001b[49m\n\u001b[0;32m     41\u001b[0m not_available \u001b[38;5;241m=\u001b[39m [div \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m all_divs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m div\u001b[38;5;241m.\u001b[39ma]\n\u001b[0;32m     42\u001b[0m not_available \u001b[38;5;241m=\u001b[39m [[div\u001b[38;5;241m.\u001b[39mget_text() \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m divs\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] ]\u001b[38;5;28;01mfor\u001b[39;00m divs \u001b[38;5;129;01min\u001b[39;00m not_available]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
     ]
    }
   ],
   "source": [
    "# download_from_level('presidential','2023-02-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c267661-3c70-4310-87ff-c1b250fe0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# # lists = page2.findAll('a', {'href': re.compile(r\"(https.*inec.*)\")} )\n",
    "# # print(len(lists))\n",
    "# h2 = page2.findAll('h2')[-1]\\\n",
    "# all_divs = h2.next_sibling.next_sibling.contents\n",
    "# # print(all_divs[-1] == ' ')\n",
    "\n",
    "# not_available = [div for div in all_divs if not div.a]\n",
    "# details = [[div.get_text() for div in divs.findAll('div')[-2:] ]for divs in not_available]\n",
    "# # for divds in not_available:\n",
    "# #     print('hello')\n",
    "#     # print([div.get_text() for div in divs])\n",
    "#     # print([div.get_text() for div in divds])\n",
    "# # not_available[0].findAll('div')[-2:]\n",
    "# # for link in lists:\n",
    "# #     img_link = link['href']\n",
    "# #     print(img_link)\n",
    "# #     print()\n",
    "#     # divs = link.parent.previous_sibling.findAll('div')\n",
    "#     # details = [div.get_text() for div in divs]\n",
    "#     # print(img_link)\n",
    "#     # print(details)\n",
    "    \n",
    "#     # break\n",
    "# # page2.find('div','text'==details)\n",
    "# print(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56bb2c-0ae3-4503-aacb-072b726b724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://inecelectionresults.ng/elections/6407df50ce35006e9218bd6e/context/ward/lga/5f0f39ad4d89fc3a883de3b9\n",
    "\n",
    "https://inecelectionresults.ng/elections/63f8f25b594e164f8146a213/context/ward/lga/5f0f39ad4d89fc3a883de3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd60bc5-77a2-4501-a83f-4a13c1ddf600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = 'https://inecelectionresults.ng/elections/63f8f25b594e164f8146a213?state=1'\n",
    "# absolute_url = urlparse(check)\n",
    "# base_url = f'{absolute_url.scheme}://{absolute_url.hostname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95489c-bf19-4361-8811-e84b652d1cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d91c86-063a-4991-b7c4-3ebb1fef371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"presidential\"\n",
    "date = '2023-02-25'\n",
    "file_base_name = f'{type}_{date}.json'\n",
    "json_data = read_json(file_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e45b2-971c-4583-b01b-03b14f579f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_keys = json_data['lgs']\n",
    "# lg_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203b680-f6b9-4f51-ab8c-39701ecc84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lg = []\n",
    "for lg in lg_keys:\n",
    "    try:\n",
    "        # temp = []\n",
    "        for ward,ward_data in json_data['downloads'][lg].items():\n",
    "            if ward not in ['status','missing']:\n",
    "                for unit,link in ward_data.items():\n",
    "                    data = []\n",
    "                    lg_name,state = lg.split('-')\n",
    "                    *ward_name,_ = ward.split('-')\n",
    "                    data.extend([state,lg_name,''.join(ward_name)])\n",
    "                    if unit == 'unavailable':\n",
    "                        for detail in link:\n",
    "                            if detail:\n",
    "                                pass\n",
    "                                # data.appen(f'{detail[1]}-->{detail[0]}')\n",
    "                    elif unit not in ['missing','recheck']:\n",
    "                        data.extend([unit,link])\n",
    "                    if len(data)>3:\n",
    "                        data_lg.append(data)\n",
    "                        \n",
    "                            \n",
    "                \n",
    "        break\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70e167-0a5e-4b69-a780-04b4d6c8965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c2ee7-1c2c-4035-9ef6-d3eb4cf2c8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e49e4-cc73-43bc-a1f9-d51f83d565e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44871af2-26dd-4e67-97db-2938b8dd99cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cdbad-745d-4242-9952-80447df02a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abdfad-1d89-47bf-8dec-95bce49a5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc96c8e-2707-4607-b0d8-7f33dc84a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6594b-f17a-4643-a98b-6eb562f29846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# querry = f'{type} election'.lower()\n",
    "# driver = webdriver.Firefox()\n",
    "# driver.get(url)\n",
    "# time.sleep(5)\n",
    "# htmlSource = driver.page_source\n",
    "# soup = BeautifulSoup(htmlSource, 'html.parser') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674868-f55b-4f2b-ab83-d9afdedf1763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821096e7-a996-4555-9889-321b7d5d45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# soup.findAll(string = 'Presidential election')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f0b7f-5a9a-4651-8132-d846eb37da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for td in soup.findAll('a'):\n",
    "#     print(td.get_text())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595992f-9d28-4c8d-a9a0-0982e0622195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for div in soup.findAll('div'):\n",
    "#     print(div)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b6f30-e14c-4993-936f-d5ef1bb4d4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c850bd-7518-4ca0-bc69-06b56f02f234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e1643-fe7e-4488-8ebe-44993b36a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
